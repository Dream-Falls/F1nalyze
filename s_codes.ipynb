{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HceoJvEP8xcj",
        "outputId": "f4b12abc-a406-4c87-9115-0e03ed7a15ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading f1-analyze-dataset, 57574882 bytes compressed\n",
            "[==================================================] 57574882 bytes downloaded\n",
            "Downloaded and uncompressed: f1-analyze-dataset\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'f1-analyze-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5298232%2F8808953%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240628%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240628T082656Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3cc91bdda13e67dd6b0392b05482d161da6d977ee714eee500a0ed5d3f36e2d3e1c2229c95a522e7f7cc70863f591533fde8983da6f82a18600d679ee9d3c3427d9612e93dbc044ec195c2a567f001ce780ba9f0aa86309744cb65fe8bd86ea4065c0f3700b52b5da722fd698b6c371cc7b4aa1b8552a28f362b035fe1b9c8b5fb05584409fcc72627ac53271c043dc3ef4317183c30a9dc01e444e921f8e2d5b4396d17163c246f548e1f788b89057bec36680233a7b5417fe8311e90b9eb5c027e14e2ee895295115312bc3654686335442d45b7ec427e6b860b161089fc7b1fead24562d1dbf9bc91baba58c59054a82669aa523f4981cad6ec21f26378fb'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBRegressor\n",
        "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Load the datasets\n",
        "X = pd.read_csv(\"/kaggle/input/f1-analyze-dataset/train.csv\")\n",
        "p = pd.read_csv(\"/kaggle/input/f1-analyze-dataset/test.csv\")\n",
        "v = pd.read_csv(\"/kaggle/input/f1-analyze-dataset/validation.csv\")\n",
        "\n",
        "columns_to_drop = [\n",
        "    'fp1_date','position','number','driverRef','driver_num','driver_code','forename','surname',\n",
        "    'fp1_time', 'fp2_date', 'fp2_time', 'fp3_date', 'fp3_time',\n",
        "    'quali_date', 'quali_time', 'sprint_date', 'sprint_time', 'url_x', 'url_y',\n",
        "    'url', 'positionText_y', 'positionText_x', 'position_x', 'grand_prix',\n",
        "    'status', 'nationality_y', 'constructorRef', 'company', 'dob', 'nationality',\n",
        "    'result_driver_standing', 'resultId', 'raceId_y', 'timetaken_in_millisec',\n",
        "    'fastestLapTime', 'time_x', 'fastestLap', 'max_speed', 'date', 'time_y'\n",
        "]\n",
        "t1 = X['position']\n",
        "X.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "t2 = v['position']\n",
        "v.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "X['rank'] = X['rank'].replace('\\\\N', np.nan)\n",
        "X['rank'] = pd.to_numeric(X['rank'], errors='coerce')\n",
        "\n",
        "# Convert to nullable integer type\n",
        "X['rank'] = X['rank'].astype('Int64')\n",
        "\n",
        "pipe = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer(max_iter=100, tol=0.001, random_state=42)),  # Impute missing values with mean\n",
        "    ('scaler', StandardScaler())  # Standardize features\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "         ('pi',pipe, X.columns) # Apply transformation pipeline to features\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "IvtnpXGd87Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a hyperparameter search space for BaggingCLassifier and XGBClassifier\n",
        "space = {\n",
        "        'n_estimators' : hp.choice('n_estimators', [500, 550 ,600 ,650 ,700,750,800]),\n",
        "        'max_depth': hp.choice('max_depth', range(2, 13)),\n",
        "        'learning_rate': hp.uniform('learning_rate', 0.0005, 0.002),\n",
        "        'min_child_weight': hp.uniform('min_child_weight', 1, 18),\n",
        "        'gamma': hp.loguniform('gamma', low=np.log(0.001), high=np.log(5)),  # Gamma from 0 to 5 (log-uniform)\n",
        "        'reg_alpha': hp.loguniform('reg_alpha', low=np.log(0.0001), high=np.log(0.8)),  # Alpha from 0 to 0.8 (log-uniform)\n",
        "        'reg_lambda': hp.loguniform('reg_lambda', low=np.log(1), high=np.log(5)),\n",
        "        'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
        "        'colsample_bynode': hp.uniform('colsample_bynode', 0.6, 1.0),\n",
        "        'colsample_bylevel': hp.uniform('colsample_bylevel', 0.6, 1.0)\n",
        "    }"
      ],
      "metadata": {
        "id": "Rs9hq3jj8_fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pre = preprocessor.fit_transform(X)\n",
        "V_pre = preprocessor.transform(v)"
      ],
      "metadata": {
        "id": "H6tlhmQu9AMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pre.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnWGlYej97qZ",
        "outputId": "91cd16c7-088c-4023-8248-392c38e1ff34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2830101, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(space):\n",
        "\n",
        "    # Flatten the target variable y\n",
        "    y_data  = t1.values.ravel()\n",
        "    # Define the BaggingClassifier with XGBClassifier as the base estimator\n",
        "    model = XGBRegressor(device = 'cuda' ,\n",
        "                        random_state=42, **space, objective='reg:squarederror',n_jobs = -1)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_pre, y_data)\n",
        "    y2_pred = model.predict(V_pre)\n",
        "    rmse = np.sqrt(mean_squared_error(t2.values.ravel(),y2_pred))\n",
        "\n",
        "    # Return loss (negative mean accuracy) and optimization status\n",
        "    return {'loss': rmse, 'status': STATUS_OK}\n",
        "\n",
        "# Run hyperparameter optimization using Hyperopt\n",
        "trials = Trials()\n",
        "best_params = fmin(objective, space, rstate=np.random.default_rng(42), algo=tpe.suggest,\n",
        "                   max_evals=100, trials=trials)\n",
        "# Print best hyperparameters found\n",
        "print(\"Best hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w87Uyk0y9F49",
        "outputId": "059c70eb-5db1-4567-fc6f-31b3ea4360d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 100/100 [1:01:47<00:00, 37.07s/trial, best loss: 2.7162468485240243]\n",
            "Best hyperparameters: {'colsample_bylevel': 0.7829573243380741, 'colsample_bynode': 0.8409512818863321, 'colsample_bytree': 0.9380717847017378, 'gamma': 1.6608611376082092, 'learning_rate': 0.0018821333068856992, 'max_depth': 9, 'min_child_weight': 17.888457276697828, 'n_estimators': 5, 'reg_alpha': 0.010806799409259934, 'reg_lambda': 4.552090852516414}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop_test = [\n",
        "    'fp1_date','number','driverRef','driver_num','driver_code','forename','surname',\n",
        "    'fp1_time', 'fp2_date', 'fp2_time', 'fp3_date', 'fp3_time',\n",
        "    'quali_date', 'quali_time', 'sprint_date', 'sprint_time', 'url_x', 'url_y',\n",
        "    'url', 'positionText_x', 'position_x', 'grand_prix',\n",
        "    'status', 'nationality_y', 'constructorRef', 'company', 'dob', 'nationality',\n",
        "    'result_driver_standing', 'resultId', 'raceId_y', 'timetaken_in_millisec',\n",
        "    'fastestLapTime', 'time_x', 'fastestLap', 'max_speed', 'date', 'time_y'\n",
        "]\n",
        "ID = p['result_driver_standing']\n",
        "p.drop(columns = columns_to_drop_test, inplace = True)\n",
        "final = preprocessor.transform(p)"
      ],
      "metadata": {
        "id": "yR9vtCIMD3M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = { 'colsample_bylevel': 0.7829573243380741, 'colsample_bynode': 0.8409512818863321,\n",
        "               'colsample_bytree': 0.9380717847017378, 'gamma': 1.6608611376082092, 'learning_rate': 0.0018821333068856992, 'max_depth': 11,\n",
        "                'min_child_weight': 17.888457276697828, 'n_estimators': 750, 'reg_alpha': 0.010806799409259934, 'reg_lambda': 4.552090852516414}\n",
        "model = XGBRegressor(device = 'cuda' ,\n",
        "                    random_state=42, **best_params, objective='reg:squarederror',n_jobs = -1)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_pre, t1.values.ravel())\n",
        "test_pred = model.predict(final)"
      ],
      "metadata": {
        "id": "oAzNfhpXDcaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df = pd.DataFrame({\n",
        "    'position' : test_pred,\n",
        "    'result_driver_standing': ID\n",
        "})\n",
        "\n",
        "# Save results to CSV file\n",
        "predictions_df.to_csv(\"submission_1.csv\", index=False)"
      ],
      "metadata": {
        "id": "WalldS1ZkAj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOQEK9n_FFBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}